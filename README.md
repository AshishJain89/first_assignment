# RAG Chatbot â€” Modular Version

This is a modular Retrieval-Augmented Generation (RAG) chatbot built with **FastAPI** (backend), **Streamlit** (frontend), and **FAISS** (vector store). It supports multiple embedding providers (SentenceTransformers, OpenAI, Cohere) and can be easily extended to Pinecone, Qdrant, or Weaviate.

---

## ğŸ“‚ Project Structure
```
rag_chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           # FastAPI app entry point
â”‚   â”œâ”€â”€ routes.py         # API endpoints for ingestion and querying
â”‚   â”œâ”€â”€ vectorstore.py    # FAISS vector index operations
â”‚   â”œâ”€â”€ embeddings.py     # Embedding helpers
â”‚   â”œâ”€â”€ processing.py     # PDF parsing and text chunking
â”‚   â”œâ”€â”€ generator.py      # LLM-based and fallback answer generation
â”‚   â”œâ”€â”€ logger.py         # Query/response logging
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py            # Streamlit UI for user interaction
â”œâ”€â”€ run_backend.py        # Shortcut to start backend
â”œâ”€â”€ run_frontend.py       # Shortcut to start frontend
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ .env                  # Environment variables (dummy values included)
```

---

## âš™ï¸ Installation

1. **Clone the repository**
```bash
git clone https://github.com/AshishJain89/rag_chatbot.git
cd rag_chatbot
```

2. **Create and activate a virtual environment**
```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set environment variables**
   - Copy `.env` to your project root
   - Add real API keys if using OpenAI/Cohere

---

## ğŸš€ Running the Chatbot

### 1ï¸âƒ£ Start the Backend (FastAPI)
```bash
python run_backend.py
```
Backend will be available at: **http://localhost:8000**

### 2ï¸âƒ£ Start the Frontend (Streamlit)
```bash
python run_frontend.py
```
Frontend will be available at: **http://localhost:8501**

---

## ğŸ“„ Usage

### Upload PDF (via Frontend)
- Go to the **Upload PDF** section in the frontend.
- Select a `.pdf` file.
- Click **Ingest PDF to backend** â€” the document is parsed, chunked, and stored in the FAISS index.

### Ask Questions (via Frontend)
- Enter your query in the text box.
- Click **Ask**.
- The system retrieves relevant chunks, reranks them, and generates an answer.

---

## ğŸ“¡ API Examples

### Ingest PDF
**Using `curl`:**
```bash
curl -X POST "http://localhost:8000/ingest" \
     -F "file=@example.pdf" \
     -F "source=example.pdf"
```

**Using Python `requests`:**
```python
import requests

files = {"file": ("example.pdf", open("example.pdf", "rb"), "application/pdf")}
resp = requests.post("http://localhost:8000/ingest", files=files)
print(resp.json())
```

### Query
**Using `curl`:**
```bash
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "What is this document about?", "top_k": 5}'
```

**Using Python `requests`:**
```python
import requests

payload = {"query": "What is this document about?", "top_k": 5}
resp = requests.post("http://localhost:8000/query", json=payload)
print(resp.json())
```

---

## ğŸ”§ Configuration

Modify `.env` to change settings:
```
EMBEDDING_PROVIDER=sentence-transformers  # or openai, cohere
EMBEDDING_MODEL=all-MiniLM-L6-v2
RAG_DATA_DIR=./rag_data
```

---

## ğŸ“¦ Extending

- Replace FAISS with Pinecone, Qdrant, or Weaviate in `vectorstore.py`.
- Add additional LLM backends in `generator.py`.
- Improve PDF parsing and chunking in `processing.py`.

---

## ğŸ“ Logging

All queries, answers, and retrieved contexts are logged in:
```
rag_data/logs.jsonl
```

---

## âš ï¸ Notes
- This is **not** production-ready; add authentication, error handling, and scaling mechanisms for production.
- Avoid storing API keys in plain `.env` in production â€” use a secrets manager.
